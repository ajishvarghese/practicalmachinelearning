<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Practicalmachinelearning by ajishvarghese</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Practicalmachinelearning</h1>
      <h2 class="project-tagline">Repo for practical machine learning final project </h2>
      <a href="https://github.com/ajishvarghese/practicalmachinelearning" class="btn">View on GitHub</a>
      <a href="https://github.com/ajishvarghese/practicalmachinelearning/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/ajishvarghese/practicalmachinelearning/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <p>&lt;!DOCTYPE html&gt;</p>



<p>
</p>



<p></p>Practical Machine Learning - Prediction Project


body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 12px;
   margin: 8px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 { 
   font-size:2.2em; 
}

h2 { 
   font-size:1.8em; 
}

h3 { 
   font-size:1.4em; 
}

h4 { 
   font-size:1.0em; 
}

h5 { 
   font-size:0.9em; 
}

h6 { 
   font-size:0.8em; 
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre {   
   margin-top: 0;
   max-width: 95%;
   border: 1px solid #ccc;
   white-space: pre-wrap;
}

pre code {
   display: block; padding: 0.5em;
}

code.r, code.cpp {
   background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

<a href="https://github.com/media" class="user-mention">@media</a> print {
   * { 
      background: transparent !important; 
      color: black !important; 
      filter:none !important; 
      -ms-filter: none !important; 
   }

   body { 
      font-size:12pt; 
      max-width:100%; 
   }
       
   a, a:visited { 
      text-decoration: underline; 
   }

   hr { 
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote { 
      padding-right: 1em; 
      page-break-inside: avoid; 
   }

   tr, img { 
      page-break-inside: avoid; 
   }

   img { 
      max-width: 100% !important; 
   }

   <a href="https://github.com/page" class="user-mention">@page</a> :left { 
      margin: 15mm 20mm 15mm 10mm; 
   }
     
   <a href="https://github.com/page" class="user-mention">@page</a> :right { 
      margin: 15mm 10mm 15mm 20mm; 
   }

   p, h2, h3 { 
      orphans: 3; widows: 3; 
   }

   h2, h3 { 
      page-break-after: avoid; 
   }
}






   pre .operator,
   pre .paren {
     color: rgb(104, 118, 135)
   }

   pre .literal {
     color: rgb(88, 72, 246)
   }

   pre .number {
     color: rgb(0, 0, 205);
   }

   pre .comment {
     color: rgb(76, 136, 107);
   }

   pre .keyword {
     color: rgb(0, 0, 255);
   }

   pre .identifier {
     color: rgb(0, 0, 0);
   }

   pre .string {
     color: rgb(3, 106, 7);
   }






<p></p>

<p></p>

<h2>
<a id="practical-machine-learning---prediction-project" class="anchor" href="#practical-machine-learning---prediction-project" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Practical Machine Learning - Prediction Project</h2>

<h1></h1>

<p>The goal of the project is to use Weight Lifing Exercise data gathered from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to classify barbell lifts into 5 different types of doing the exercise correctly or incorrectly. To achieve this goal, I will build a prediction model using the random forests algorithm which is widely recognised for its high accuracy for these type of classification problems.</p>

<h2>
<a id="approach" class="anchor" href="#approach" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Approach</h2>

<h1></h1>

<h3>
<a id="dataset-exploration-and-cleaning" class="anchor" href="#dataset-exploration-and-cleaning" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Dataset Exploration and Cleaning</h3>

<p>The first step is to examine the training and testing (prediction) datasets. </p>

<pre><code>datatr&lt;-read.csv("pml-training.csv")
datatst&lt;-read.csv("pml-testing.csv")
</code></pre>

<p>The datasets have 19622 and 20 observations of 160 variables.</p>

<pre><code>dim(datatr)
</code></pre>

<pre><code>## [1] 19622   160
</code></pre>

<pre><code>dim(datatst)
</code></pre>

<pre><code>## [1]  20 160
</code></pre>

<p>Both have the same variables except for the last one, the classe response variable in the training set but problem_id in the prediction set.</p>

<pre><code>which(!names(datatr)==names(datatst))
</code></pre>

<pre><code>## [1] 160
</code></pre>

<pre><code>names(datatr)[160]
</code></pre>

<pre><code>## [1] "classe"
</code></pre>

<pre><code>names(datatst)[160]
</code></pre>

<pre><code>## [1] "problem_id"
</code></pre>

<p>Using the summary function on the datasets indicate a many sparsely populated columns with the occurences of 'NA', blanks and '#DIV/0!' (which should be converted to NA for analysis). The sparseness is examined to determine its extent and the further action warranted.  </p>

<pre><code>fullcoltr&lt;-apply(datatr,2,function(x) sum(!is.na(x)))
nacoltr&lt;-apply(datatr,2,function(x) sum(is.na(x)))
fullrowtr&lt;-apply(datatr,1,function(x) sum(!is.na(x)))
narowtr&lt;-apply(datatr,1,function(x) sum(is.na(x)))
unique(fullcoltr)
</code></pre>

<pre><code>## [1] 19622   406
</code></pre>

<pre><code>unique(nacoltr)
</code></pre>

<pre><code>## [1]     0 19216
</code></pre>

<pre><code>unique(fullrowtr)
</code></pre>

<pre><code>## [1]  93 160
</code></pre>

<pre><code>unique(narowtr) 
</code></pre>

<pre><code>## [1] 67  0
</code></pre>

<p>67 variables in the training set are NA for all observations, the other observations are fully populated except for 406 cases.</p>

<pre><code>fullcoltst&lt;-apply(datatst,2,function(x) sum(!is.na(x)))
nacoltst&lt;-apply(datatst,2,function(x) sum(is.na(x)))
fullrowtst&lt;-apply(datatst,1,function(x) sum(!is.na(x)))
narowtst&lt;-apply(datatst,1,function(x) sum(is.na(x)))
unique(fullcoltst)
</code></pre>

<pre><code>## [1] 20  0
</code></pre>

<pre><code>unique(nacoltst)
</code></pre>

<pre><code>## [1]  0 20
</code></pre>

<pre><code>unique(fullrowtst)
</code></pre>

<pre><code>## [1] 60
</code></pre>

<pre><code>unique(narowtst)
</code></pre>

<pre><code>## [1] 100
</code></pre>

<p>The prediction set has 100/160 NA columns for the 20 observations. Therefore, the choice of predictor columns can be narrowed down to the fully populated columns present in the prediction (and training) sets.  </p>

<pre><code>data&lt;-read.csv("pml-training.csv",na.strings = c("NA","","#DIV/0!"))
p_data&lt;-read.csv("pml-testing.csv",na.strings = c("NA","","#DIV/0!"))
fullcoldata&lt;-apply(data,2,function(x) sum(!is.na(x)))
colsel&lt;-which(fullcoldata==nrow(data))
fullcolpdata&lt;-apply(p_data,2,function(x) sum(!is.na(x)))
colselp&lt;-which(fullcolpdata==nrow(p_data))
</code></pre>

<p>Again, we see that both sets have 60 fully populated variables including the last one (classe/problem_id) that is different between the two data sets. Also, the first seven variables are not related to the accelerometer data and excluded as predictors.</p>

<pre><code>length(colsel)
</code></pre>

<pre><code>## [1] 60
</code></pre>

<pre><code>sum(colsel==colselp)
</code></pre>

<pre><code>## [1] 60
</code></pre>

<pre><code>sum(names(colsel)==names(colselp))
</code></pre>

<pre><code>## [1] 59
</code></pre>

<pre><code>names(colsel)[1:7]
</code></pre>

<pre><code>## [1] "X"                    "user_name"            "raw_timestamp_part_1"
## [4] "raw_timestamp_part_2" "cvtd_timestamp"       "new_window"          
## [7] "num_window"
</code></pre>

<pre><code>colsel&lt;-colsel[-c(1:7)]
</code></pre>

<p>The datasets can now be subset to get the data sets to be used for model building and prediction.</p>

<pre><code>compldata&lt;-data[,colsel]
complp_data&lt;-p_data[,colsel]
</code></pre>

<h3>
<a id="model-building" class="anchor" href="#model-building" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Model building</h3>

<p>The caret package is used for data slicing into training and building the random forest classification model including cross-validation. The training data is subset into two data sets (60/40, with seed set for reproducibility) into a new 'training' set to train the model and an independent 'test' set to provide an unbiased validation of the model and an estimate of the out of sample error.</p>

<pre><code>library(caret)
</code></pre>

<pre><code>## Loading required package: lattice
## Loading required package: ggplot2
</code></pre>

<pre><code>set.seed(1011)
inTrain = createDataPartition(compldata$classe, p = 0.6)[[1]]
training = compldata[ inTrain,]
testing = compldata[-inTrain,]
</code></pre>

<p>The data set has 52 predictor variables whose interrelationships (such as collinearity) and predictive potential are not known.Random forests is able to handle such cases with a high degree of accuracy. Moreover, it automatically calculates the variable importance in prediction and includes cross-validation during the forest building progress, calculating the out of bag (oob) error rate. Further pre-processing of data is not necessary in this method.</p>

<pre><code>modFitrf &lt;- train(classe ~ .,data=training,method="rf",prox=FALSE)
</code></pre>

<pre><code>## Loading required package: randomForest
## randomForest 4.6-10
## Type rfNews() to see new features/changes/bug fixes.
</code></pre>

<pre><code>modFitrf
</code></pre>

<pre><code>## Random Forest 
## 
## 11776 samples
##    52 predictor
##     5 classes: 'A', 'B', 'C', 'D', 'E' 
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## 
## Summary of sample sizes: 11776, 11776, 11776, 11776, 11776, 11776, ... 
## 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa      Accuracy SD  Kappa SD   
##    2    0.9846881  0.9806284  0.002383020  0.003001932
##   27    0.9864338  0.9828370  0.002473017  0.003126635
##   52    0.9770509  0.9709670  0.004810608  0.006091112
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was mtry = 27.
</code></pre>

<h3>
<a id="model-evaluation" class="anchor" href="#model-evaluation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Model evaluation</h3>

<p>The model used all 52 predictors and an mtry of 27 predictors in the random forest, with an overall Kappa of 0.9864. Applying the trained model to predicting the 'classe' on the testing set gives an out of sample error of just under 0.78%. The plot of variable importance is also obtained from the random forest as seen below.</p>

<pre><code>prf &lt;- predict(modFitrf,testing)
ooserr &lt;- sum(testing$classe!=prf)/nrow(testing)
paste("Out of sample error: ", ooserr)
</code></pre>

<pre><code>## [1] "Out of sample error:  0.00777466224827938"
</code></pre>

<pre><code>plot(varImp(modFitrf), main="Variable ImportancePlot", ylab="Variables")
</code></pre>

<p><img alt="plot of chunk unnamed-chunk-11"> </p>

<p></p>

<p></p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/ajishvarghese/practicalmachinelearning">Practicalmachinelearning</a> is maintained by <a href="https://github.com/ajishvarghese">ajishvarghese</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
